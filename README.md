# NaBiJam: AI-based Baby Cry Analysis and Chatbot Service

### 프로젝트 개요

본 프로젝트는 **아기의 울음소리를 분석하여 상태를 자동으로 분류**하는 인공지능 모델을 설계하고, 이를 기반으로 **육아 도우미 챗봇 서비스**로 확장하는 것이 목표였습니다. 특히 **응급상황의 선별적 대응**을 위해 `'pain'` 클래스를 우선적으로 분류할 수 있도록 이진 분류 모델을 우선 설계하고, 이후 `'pain'`을 제외한 다양한 상태를 분류하는 다중 분류 모델을 함께 구현했습니다.

- `first_model`: `'pain'` 클래스 유무를 판별하는 **이진 분류 모델**
- `second_model`: `'pain'`을 제외한 상태를 분류하는 **다중 분류 모델**

---

### 오디오 데이터 전처리 및 분석

### [오디오 데이터의 이해]
초기에는 오디오 데이터의 특성을 이해하기 위해 **MFCC(Mel Frequency Cepstral Coefficients)** 기반의 특징 추출을 수행하였고, 이를 통해 울음소리의 주요 패턴을 파악하고자 하였습니다.

---

### Dataset 1 분석

[데이터 특징]
- **총 샘플 수 500개 미만**, 클래스는 5개
- **극심한 불균형 (약 300:8 수준)**
- 기존 논문이나 GitHub 사례에서도 성능이 낮은 데이터로 평가됨

[실험 내용]
1. **MFCC 기반 머신러닝**
   - 정확도 약 **30%**
   - Confusion Matrix 분석 결과, 실제로는 거의 분류하지 못함

2. **Spectrogram 이미지 기반 딥러닝 (CNN)**
   - 정확도 약 **50%**
   - 다수를 차지하는 클래스만 주로 예측

[결론]
- 해당 데이터셋만으로는 **의미 있는 분류 모델 생성이 어려움**
- **추가 데이터 수집의 필요성 확인**

---

### Dataset 2 분석

[데이터 특징]
- 기존 클래스와 일치하지 않음
- 수집 및 라벨링 기준이 불분명

[실험 내용]
- Dataset1을 통해 아기 울음소리는 말소리보다 음악처럼 **복잡한 패턴을 가짐**을 인지
- 따라서 **SE-ResNet** 구조를 기반으로 Pytorch로 구현하여 실험 진행  
  (SE-ResNet 논문: [MDPI, 2024](https://www.mdpi.com/1424-8220/24/20/6575))

[실험 결과]
- 정확도 **약 20%** 수준
- 단순히 데이터셋 변경만으로는 성능 개선이 어려움
- 복잡한 모델이 오히려 **울음소리의 미세한 변화**를 잘 포착하지 못할 가능성 제기

---

### Dataset 3 분석 및 최종 모델

[데이터 특징]
- 기존 데이터셋 한계를 보완하기 위해 [이 GitHub 저장소](https://github.com/Aman-Vishwakarma1729/Automatic_Infant_Cry_Audio_Classification) 참고하여 **추가 데이터 수집**
- 전처리가 완료된 형태로 구성됨

[전처리 및 모델 구성]
- 아기 울음은 녹음 길이에 따라 입력 길이가 상이하므로, **MFCC(Mel Frequency Cepstral Coefficients)**를 활용하여 입력 길이를 고정된 형태의 벡터로 변환함으로써 모델의 입력 형태를 고정. 
- 아기 울음소리의 변화 패턴을 효과적으로 포착하기 위해, 전처리 과정에서 미분 연산을 적용하여 시간에 따른 변화를 강조(원본 MFCC 계수에 더해 1차 미분, 2차 미분 값을 각각 계산한 후, 세 가지 벡터를 이어붙여 하나의 입력 벡터로 구성함으로써 소리의 정적 특성과 동적 변화를 동시에 반영)
- 추후 마이크로 컨트롤러 기반 하드웨어에서 실행할 수 있도록, 모델은 TensorFlow Lite로의 변환을 고려하여 TensorFlow 기반으로 경량화된 구조로 설계
- `first_model`: pain 클래스 이진 분류
- `second_model`: 나머지 클래스 다중 분류


[성능 결과]
- **first_model**: 약 **90% 정확도**
- **second_model**: 약 **83% 정확도**

---

### 결론 및 인사이트

- 아기 울음소리는 **정형화된 언어보다 복잡한 특성**을 가지므로 단순한 모델이나 작은 데이터셋으로는 한계가 있음
- **데이터 품질과 수집 구조**가 모델 성능에 결정적 영향을 미침
- 최종적으로는 **정제된 추가 데이터셋 확보**와 **단계적 모델 설계 전략**을 통해 실제 활용 가능한 분류 모델을 구축할 수 있었음

---

[주요 파일 구성]

| 파일명 | 설명 |
|--------|------|
| `0.오디오데이터_MFCC이해.ipynb` | 오디오 데이터에 대한 전반적인 이해 |
| `1.data1_MFCC특징추출.ipynb` | Dataset1 MFCC 기반 전처리 |
| `1.data1_MFCC_머신러닝.ipynb` | Dataset1 머신러닝 실험(벡터) |
| `1.data1_spectrogram_CNN.ipynb` | Dataset1 딥러닝 실험(이미지) |
| `2.data2_preprocess.ipynb` | Dataset2 전처리(이미지로추출) |
| `2.data2_SE-ResNet.ipynb` | Dataset2 SE-ResNet 실험(이미지 딥러닝)|
| `for_sharing_first_model.ipynb` | pain 클래스 이진 분류 최종 모델(벡터+미분) |
| `for_sharing_second_model.ipynb` | 다중 분류 최종 모델(벡터+미분)|
| `pipeline.ipynb` | 전체 프로젝트 파이프라인 정리(첫번째 모델의 결과가 비응급인 경우에만 두번째 모델의 결과 출력)|

---

